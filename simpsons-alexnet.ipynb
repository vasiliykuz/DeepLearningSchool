{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    #for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch torchvision;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:52:43.076084Z",
     "iopub.status.busy": "2021-11-21T14:52:43.075601Z",
     "iopub.status.idle": "2021-11-21T14:52:45.326051Z",
     "shell.execute_reply": "2021-11-21T14:52:45.325334Z",
     "shell.execute_reply.started": "2021-11-21T14:52:43.075999Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:52:51.394538Z",
     "iopub.status.busy": "2021-11-21T14:52:51.39424Z",
     "iopub.status.idle": "2021-11-21T14:52:51.39841Z",
     "shell.execute_reply": "2021-11-21T14:52:51.397713Z",
     "shell.execute_reply.started": "2021-11-21T14:52:51.394483Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:52:54.194459Z",
     "iopub.status.busy": "2021-11-21T14:52:54.193898Z",
     "iopub.status.idle": "2021-11-21T14:52:54.202336Z",
     "shell.execute_reply": "2021-11-21T14:52:54.201343Z",
     "shell.execute_reply.started": "2021-11-21T14:52:54.194421Z"
    }
   },
   "outputs": [],
   "source": [
    "import random \n",
    "SEED = 7\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:52:56.365419Z",
     "iopub.status.busy": "2021-11-21T14:52:56.364818Z",
     "iopub.status.idle": "2021-11-21T14:52:56.424013Z",
     "shell.execute_reply": "2021-11-21T14:52:56.423259Z",
     "shell.execute_reply.started": "2021-11-21T14:52:56.365381Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path('../input/journey-springfield/train')\n",
    "TEST_DIR = Path('../input/journey-springfield/testset')\n",
    "DATA_MODES = ['train', 'val', 'test']\n",
    "RESCALE_SIZE = 224\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "BATCH_SIZE=128\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:52:58.475234Z",
     "iopub.status.busy": "2021-11-21T14:52:58.474743Z",
     "iopub.status.idle": "2021-11-21T14:52:58.487976Z",
     "shell.execute_reply": "2021-11-21T14:52:58.487119Z",
     "shell.execute_reply.started": "2021-11-21T14:52:58.475197Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpsonsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, files, mode):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.files = sorted(files)\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.len_ = len(self.files)\n",
    "     \n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "\n",
    "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "                  pickle.dump(self.label_encoder, le_dump_file)\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        image = Image.open(file)\n",
    "        image.load()\n",
    "        return image\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "        data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),])\n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = self._prepare_sample(x)\n",
    "        x = np.array(x / 255, dtype='float32')\n",
    "        if self.mode == 'train':\n",
    "          x = data_transforms(x)\n",
    "        else:\n",
    "          x = transform(x)\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y\n",
    "        \n",
    "    def _prepare_sample(self, image):\n",
    "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
    "        return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:01.833389Z",
     "iopub.status.busy": "2021-11-21T14:53:01.833134Z",
     "iopub.status.idle": "2021-11-21T14:53:05.044654Z",
     "shell.execute_reply": "2021-11-21T14:53:05.043884Z",
     "shell.execute_reply.started": "2021-11-21T14:53:01.833362Z"
    }
   },
   "outputs": [],
   "source": [
    "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
    "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_labels = [path.parent.name for path in train_val_files]\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n",
    "                                          stratify=train_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:07.210277Z",
     "iopub.status.busy": "2021-11-21T14:53:07.210023Z",
     "iopub.status.idle": "2021-11-21T14:53:07.229226Z",
     "shell.execute_reply": "2021-11-21T14:53:07.228566Z",
     "shell.execute_reply.started": "2021-11-21T14:53:07.21025Z"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(train_val_labels))\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:09.931717Z",
     "iopub.status.busy": "2021-11-21T14:53:09.930892Z",
     "iopub.status.idle": "2021-11-21T14:53:09.978998Z",
     "shell.execute_reply": "2021-11-21T14:53:09.978319Z",
     "shell.execute_reply.started": "2021-11-21T14:53:09.931665Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = [path.parent.name for path in train_files] # классы train\n",
    "val_labels = [path.parent.name for path in val_files]     # классы val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:12.331228Z",
     "iopub.status.busy": "2021-11-21T14:53:12.330971Z",
     "iopub.status.idle": "2021-11-21T14:53:12.33806Z",
     "shell.execute_reply": "2021-11-21T14:53:12.337363Z",
     "shell.execute_reply.started": "2021-11-21T14:53:12.3312Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dct_path_labels(train_files, train_labels):\n",
    "    dct_simpsons = {}\n",
    "    for label_i in np.unique(train_labels).tolist():\n",
    "        dct_simpsons[label_i] = []\n",
    "\n",
    "    for path_i, label_i in zip(train_files, train_labels):\n",
    "        dct_simpsons[label_i].append(path_i)\n",
    "\n",
    "    return dct_simpsons\n",
    "\n",
    "def print_dct(dct_simpsons):\n",
    "    for key in dct_simpsons:\n",
    "        print(f\"{key}\\t{dct_simpsons[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:15.067371Z",
     "iopub.status.busy": "2021-11-21T14:53:15.066914Z",
     "iopub.status.idle": "2021-11-21T14:53:15.084476Z",
     "shell.execute_reply": "2021-11-21T14:53:15.08374Z",
     "shell.execute_reply.started": "2021-11-21T14:53:15.067335Z"
    }
   },
   "outputs": [],
   "source": [
    "dct_path_train = create_dct_path_labels(train_files, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:24.237677Z",
     "iopub.status.busy": "2021-11-21T14:53:24.23696Z",
     "iopub.status.idle": "2021-11-21T14:53:24.249452Z",
     "shell.execute_reply": "2021-11-21T14:53:24.248522Z",
     "shell.execute_reply.started": "2021-11-21T14:53:24.23764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Дополним картинки классов у которых менее 100 картинок, до 100 картинок в классе\n",
    "for person in dct_path_train:\n",
    "    if len(dct_path_train[person]) < 100:\n",
    "        dct_path_train[person] = dct_path_train[person] * (100 // len(dct_path_train[person]))\n",
    "        dct_path_train[person].extend(dct_path_train[person][:100 - len(dct_path_train[person])])\n",
    "\n",
    "# Проверим что получилось \n",
    "for person in dct_path_train:\n",
    "    print(f\"{person}\\t{len(dct_path_train[person])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:27.90766Z",
     "iopub.status.busy": "2021-11-21T14:53:27.906928Z",
     "iopub.status.idle": "2021-11-21T14:53:27.947784Z",
     "shell.execute_reply": "2021-11-21T14:53:27.947045Z",
     "shell.execute_reply.started": "2021-11-21T14:53:27.907621Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dct_from_labels(train_val_labels):\n",
    "    \"\"\"Функция создает из list train_val_labels, содержащего метки классов\n",
    "    соответсвующим картинкам из выборки, словарь dict с ключами соответсвующими\n",
    "    названиям классов, и значениями, соответвующими колчеству этих классов в \n",
    "    list train_val_labels\"\"\"\n",
    "    dct_simpsons = {}\n",
    "    for label_i in np.unique(train_val_labels).tolist():\n",
    "        dct_simpsons.update({label_i:train_val_labels.count(label_i)})\n",
    "\n",
    "    return dct_simpsons\n",
    "\n",
    "new_train_files = []\n",
    "\n",
    "for person in dct_path_train:\n",
    "    new_train_files.extend(dct_path_train[person])\n",
    "\n",
    "new_train_label = [path.parent.name for path in new_train_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:35.448529Z",
     "iopub.status.busy": "2021-11-21T14:53:35.447795Z",
     "iopub.status.idle": "2021-11-21T14:53:35.625124Z",
     "shell.execute_reply": "2021-11-21T14:53:35.62446Z",
     "shell.execute_reply.started": "2021-11-21T14:53:35.448472Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset = SimpsonsDataset(val_files, mode='val')\n",
    "new_train_dataset = SimpsonsDataset(new_train_files, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:38.471162Z",
     "iopub.status.busy": "2021-11-21T14:53:38.470444Z",
     "iopub.status.idle": "2021-11-21T14:53:38.478295Z",
     "shell.execute_reply": "2021-11-21T14:53:38.477415Z",
     "shell.execute_reply.started": "2021-11-21T14:53:38.471127Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_dataloader, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.to(DEVICE) #перенос тензоров на видеокарту\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad() #обнуляем градиенты, чтобы они не накапливались\n",
    "\n",
    "        outputs = model(inputs) #прогоняем данные из трейнлоадера чеез нашу сеть (модель). интересно, почему не model.forward(inputs)?\n",
    "        loss = criterion(outputs, labels) #считаем лосс\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1) #все, что больше 1, приводим к 1\n",
    "        running_loss += loss.item() * inputs.size(0) #непонятно, как работает .size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data) #если предсказание совпадает с ответом, увеличиваем на 1 running_corrects\n",
    "        processed_data += inputs.size(0) \n",
    "              \n",
    "    train_loss = running_loss / processed_data #потери на трейне\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data #точность на трейне\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:42.389876Z",
     "iopub.status.busy": "2021-11-21T14:53:42.389317Z",
     "iopub.status.idle": "2021-11-21T14:53:42.397277Z",
     "shell.execute_reply": "2021-11-21T14:53:42.396383Z",
     "shell.execute_reply.started": "2021-11-21T14:53:42.38984Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_dataloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:44.612247Z",
     "iopub.status.busy": "2021-11-21T14:53:44.61199Z",
     "iopub.status.idle": "2021-11-21T14:53:44.6244Z",
     "shell.execute_reply": "2021-11-21T14:53:44.623717Z",
     "shell.execute_reply.started": "2021-11-21T14:53:44.612221Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_files, val_files, model, epochs, batch_size):\n",
    "    train_dataloader = DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        opt = torch.optim.Adam(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_dataloader, criterion, opt)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_dataloader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:49.885793Z",
     "iopub.status.busy": "2021-11-21T14:53:49.885148Z",
     "iopub.status.idle": "2021-11-21T14:53:54.351442Z",
     "shell.execute_reply": "2021-11-21T14:53:54.350662Z",
     "shell.execute_reply.started": "2021-11-21T14:53:49.885751Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:53:57.310601Z",
     "iopub.status.busy": "2021-11-21T14:53:57.310063Z",
     "iopub.status.idle": "2021-11-21T14:54:00.047972Z",
     "shell.execute_reply": "2021-11-21T14:54:00.047248Z",
     "shell.execute_reply.started": "2021-11-21T14:53:57.310565Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_features = 9216\n",
    "model.classifier = nn.Linear(num_features, n_classes)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T14:54:03.998184Z",
     "iopub.status.busy": "2021-11-21T14:54:03.997928Z",
     "iopub.status.idle": "2021-11-21T15:15:34.982326Z",
     "shell.execute_reply": "2021-11-21T15:15:34.981532Z",
     "shell.execute_reply.started": "2021-11-21T14:54:03.998156Z"
    }
   },
   "outputs": [],
   "source": [
    "history = train(new_train_dataset, val_dataset, model=model, epochs=8, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:15:46.317365Z",
     "iopub.status.busy": "2021-11-21T15:15:46.316879Z",
     "iopub.status.idle": "2021-11-21T15:15:46.356425Z",
     "shell.execute_reply": "2021-11-21T15:15:46.355723Z",
     "shell.execute_reply.started": "2021-11-21T15:15:46.317331Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Simpsons_AlexNet.pth')\n",
    "model.load_state_dict(torch.load('Simpsons_AlexNet.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:15:48.951576Z",
     "iopub.status.busy": "2021-11-21T15:15:48.950889Z",
     "iopub.status.idle": "2021-11-21T15:15:48.958012Z",
     "shell.execute_reply": "2021-11-21T15:15:48.957233Z",
     "shell.execute_reply.started": "2021-11-21T15:15:48.951534Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:15:53.413478Z",
     "iopub.status.busy": "2021-11-21T15:15:53.412918Z",
     "iopub.status.idle": "2021-11-21T15:15:53.64707Z",
     "shell.execute_reply": "2021-11-21T15:15:53.646339Z",
     "shell.execute_reply.started": "2021-11-21T15:15:53.413438Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:15:57.27425Z",
     "iopub.status.busy": "2021-11-21T15:15:57.273804Z",
     "iopub.status.idle": "2021-11-21T15:15:57.493955Z",
     "shell.execute_reply": "2021-11-21T15:15:57.493247Z",
     "shell.execute_reply.started": "2021-11-21T15:15:57.274215Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acc, label=\"train_acc\")\n",
    "plt.plot(val_acc, label=\"val_acc\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вижу потенциал, поучу еще"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:16:38.292859Z",
     "iopub.status.busy": "2021-11-21T15:16:38.29259Z",
     "iopub.status.idle": "2021-11-21T15:37:02.582821Z",
     "shell.execute_reply": "2021-11-21T15:37:02.582013Z",
     "shell.execute_reply.started": "2021-11-21T15:16:38.292829Z"
    }
   },
   "outputs": [],
   "source": [
    "history = train(new_train_dataset, val_dataset, model=model, epochs=8, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:37:40.797568Z",
     "iopub.status.busy": "2021-11-21T15:37:40.797276Z",
     "iopub.status.idle": "2021-11-21T15:37:40.842848Z",
     "shell.execute_reply": "2021-11-21T15:37:40.842205Z",
     "shell.execute_reply.started": "2021-11-21T15:37:40.797537Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Simpsons_AlexNet.pth')\n",
    "model.load_state_dict(torch.load('Simpsons_AlexNet.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:37:43.571301Z",
     "iopub.status.busy": "2021-11-21T15:37:43.571039Z",
     "iopub.status.idle": "2021-11-21T15:37:43.57507Z",
     "shell.execute_reply": "2021-11-21T15:37:43.574281Z",
     "shell.execute_reply.started": "2021-11-21T15:37:43.571264Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:37:45.805611Z",
     "iopub.status.busy": "2021-11-21T15:37:45.805233Z",
     "iopub.status.idle": "2021-11-21T15:37:45.811019Z",
     "shell.execute_reply": "2021-11-21T15:37:45.810136Z",
     "shell.execute_reply.started": "2021-11-21T15:37:45.805576Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    with torch.no_grad(): #без градиентов\n",
    "        logits = []\n",
    "    \n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:37:55.609Z",
     "iopub.status.busy": "2021-11-21T15:37:55.608745Z",
     "iopub.status.idle": "2021-11-21T15:37:55.615987Z",
     "shell.execute_reply": "2021-11-21T15:37:55.613787Z",
     "shell.execute_reply.started": "2021-11-21T15:37:55.608972Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_one_sample(model, inputs, device=DEVICE):\n",
    "    \"\"\"Предсказание, для одной картинки\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        logit = model(inputs).cpu()\n",
    "        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:38:01.173092Z",
     "iopub.status.busy": "2021-11-21T15:38:01.172326Z",
     "iopub.status.idle": "2021-11-21T15:38:01.502978Z",
     "shell.execute_reply": "2021-11-21T15:38:01.502266Z",
     "shell.execute_reply.started": "2021-11-21T15:38:01.173049Z"
    }
   },
   "outputs": [],
   "source": [
    "random_characters = int(np.random.uniform(0,1000))\n",
    "ex_img, true_label = val_dataset[random_characters]\n",
    "probs_im = predict_one_sample(model, ex_img.unsqueeze(0))\n",
    "\n",
    "idxs = list(map(int, np.random.uniform(0,1000, 20)))\n",
    "imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n",
    "\n",
    "probs_ims = predict(model, imgs)\n",
    "\n",
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
    "\n",
    "y_pred = np.argmax(probs_ims,-1)\n",
    "\n",
    "actual_labels = [val_dataset[id][1] for id in idxs]\n",
    "\n",
    "preds_class = [label_encoder.classes_[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:38:04.999178Z",
     "iopub.status.busy": "2021-11-21T15:38:04.998925Z",
     "iopub.status.idle": "2021-11-21T15:38:11.104067Z",
     "shell.execute_reply": "2021-11-21T15:38:11.103347Z",
     "shell.execute_reply.started": "2021-11-21T15:38:04.99915Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
    "probs = predict(model, test_loader) #матрица с вероятностями для каждого класса\n",
    "\n",
    "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1)) #вектор в наибольшими вероятностями\n",
    "test_filenames = [path.name for path in test_dataset.files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T15:38:17.047189Z",
     "iopub.status.busy": "2021-11-21T15:38:17.046689Z",
     "iopub.status.idle": "2021-11-21T15:38:17.079448Z",
     "shell.execute_reply": "2021-11-21T15:38:17.078029Z",
     "shell.execute_reply.started": "2021-11-21T15:38:17.047145Z"
    }
   },
   "outputs": [],
   "source": [
    "# ДОБАВЛЕНО: создание сабмита\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['Id'] = test_filenames\n",
    "df['Expected'] = preds\n",
    "df.to_csv(Path('./submission.csv'), index=False)\n",
    "# файл появится у вас на гугл диске"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
